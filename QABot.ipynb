{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvSGDbExff_I"
   },
   "source": [
    "# QABot with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGOr_eS3wJJf"
   },
   "source": [
    "## Task description\n",
    "- Extractive Question Answering\n",
    "  - Input: Paragraph + Question\n",
    "  - Output: Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dKM4yCh4LI_"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WOTHHtWJoahe"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from torch.optim import AdamW\n",
    "from transformers import BertForQuestionAnswering, BertTokenizerFast, get_cosine_schedule_with_warmup\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "def same_seeds(seed):\n",
    "\t  torch.manual_seed(seed)\n",
    "\t  if torch.cuda.is_available():\n",
    "\t\t    torch.cuda.manual_seed(seed)\n",
    "\t\t    torch.cuda.manual_seed_all(seed)\n",
    "\t  np.random.seed(seed)\n",
    "\t  random.seed(seed)\n",
    "\t  torch.backends.cudnn.benchmark = False\n",
    "\t  torch.backends.cudnn.deterministic = True\n",
    "same_seeds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7pBtSZP1SKQO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\n",
    "fp16_training = True\n",
    "\n",
    "if fp16_training:\n",
    "    # !pip install accelerate==0.2.0\n",
    "    from accelerate import Accelerator\n",
    "    accelerator = Accelerator(mixed_precision='fp16')\n",
    "    device = accelerator.device\n",
    "    print(device)\n",
    "\n",
    "# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YgXHuVLp_6j"
   },
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xyBCYGjAp3ym"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased').to(device)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Td-GTmk5OW4"
   },
   "source": [
    "## Read Data\n",
    "\n",
    "- Training set: 87599 QA pairs\n",
    "  - max tokenized question length: 61\n",
    "  - max tokenized context length: 853\n",
    "- Dev set: 34726  QA pairs\n",
    "- Test set: try to generate via ChatGPT(to-do)\n",
    "\n",
    "\n",
    "dataset structure\n",
    "- title\n",
    "  - paragraphs\n",
    "    - context\n",
    "    - qas\n",
    "      - answers\n",
    "        - answer_start\n",
    "        - test\n",
    "      - question\n",
    "      - id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    with open(file, 'rb') as reader:\n",
    "        data = json.load(reader)\n",
    "    contexts, questions, answers = [], [], []\n",
    "    for group in data['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "    return contexts, questions, answers\n",
    "\n",
    "train_contexts, train_questions, train_answers = read_data(\"dataset/train-v1.1.json\")\n",
    "dev_contexts, dev_questions, dev_answers = read_data(\"dataset/dev-v1.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87599"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34726"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_contexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_start': 515, 'text': 'Saint Bernadette Soubirous'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_answers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mutiple answers to the same question in dev set, I guess it's for the accuracy of labeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Which NFL team represented the AFC at Super Bowl 50?',\n",
       " 'Which NFL team represented the AFC at Super Bowl 50?',\n",
       " 'Which NFL team represented the AFC at Super Bowl 50?',\n",
       " 'Which NFL team represented the NFC at Super Bowl 50?',\n",
       " 'Which NFL team represented the NFC at Super Bowl 50?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer_start': 177, 'text': 'Denver Broncos'},\n",
       " {'answer_start': 177, 'text': 'Denver Broncos'},\n",
       " {'answer_start': 177, 'text': 'Denver Broncos'},\n",
       " {'answer_start': 249, 'text': 'Carolina Panthers'},\n",
       " {'answer_start': 249, 'text': 'Carolina Panthers'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_answers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where did Super Bowl 50 take place?',\n",
       " 'Where did Super Bowl 50 take place?',\n",
       " 'Where did Super Bowl 50 take place?']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_questions[6:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer_start': 403, 'text': 'Santa Clara, California'},\n",
       " {'answer_start': 355, 'text': \"Levi's Stadium\"},\n",
       " {'answer_start': 355,\n",
       "  'text': \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_answers[6:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Which NFL team won Super Bowl 50?',\n",
       " 'Which NFL team won Super Bowl 50?',\n",
       " 'What color was used to emphasize the 50th anniversary of the Super Bowl?',\n",
       " 'What color was used to emphasize the 50th anniversary of the Super Bowl?',\n",
       " 'What color was used to emphasize the 50th anniversary of the Super Bowl?']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_questions[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer_start': 177, 'text': 'Denver Broncos'},\n",
       " {'answer_start': 177, 'text': 'Denver Broncos'},\n",
       " {'answer_start': 488, 'text': 'gold'},\n",
       " {'answer_start': 488, 'text': 'gold'},\n",
       " {'answer_start': 521, 'text': 'gold'}]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_answers[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fm0rpTHq0e4N"
   },
   "source": [
    "## Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rTZ6B70Hoxie"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Tokenize questions and paragraphs separatelymixed_precision\n",
    "# 「add_special_tokens」 is set to False since special tokens will be added when tokenized questions and paragraphs are combined in datset __getitem__\n",
    "\n",
    "train_questions_tokenized = tokenizer([train_question for train_question in train_questions], add_special_tokens=False)\n",
    "# dev_questions_tokenized = tokenizer([dev_question for dev_question in dev_questions], add_special_tokens=False)\n",
    "\n",
    "train_contexts_tokenized = tokenizer(train_contexts, add_special_tokens=False)\n",
    "# dev_contexts_tokenized = tokenizer(dev_contexts, add_special_tokens=False)\n",
    "\n",
    "\n",
    "# tokenized sequences will be futher processed in datset __getitem__ before passing to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87599"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_questions_tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87599"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max tokenized question length\n",
    "max(len(train_questions_tokenized['input_ids'][i]) for i in range(len(train_questions_tokenized['input_ids'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "853"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max tokenized question length\n",
    "max(len(train_contexts_tokenized['input_ids'][i]) for i in range(len(train_contexts_tokenized['input_ids'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_answers_tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_contexts_tokenized.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6549,\n",
       " 2135,\n",
       " 1010,\n",
       " 1996,\n",
       " 2082,\n",
       " 2038,\n",
       " 1037,\n",
       " 3234,\n",
       " 2839,\n",
       " 1012,\n",
       " 10234,\n",
       " 1996,\n",
       " 2364,\n",
       " 2311,\n",
       " 1005,\n",
       " 1055,\n",
       " 2751,\n",
       " 8514,\n",
       " 2003,\n",
       " 1037,\n",
       " 3585,\n",
       " 6231,\n",
       " 1997,\n",
       " 1996,\n",
       " 6261,\n",
       " 2984,\n",
       " 1012,\n",
       " 3202,\n",
       " 1999,\n",
       " 2392,\n",
       " 1997,\n",
       " 1996,\n",
       " 2364,\n",
       " 2311,\n",
       " 1998,\n",
       " 5307,\n",
       " 2009,\n",
       " 1010,\n",
       " 2003,\n",
       " 1037,\n",
       " 6967,\n",
       " 6231,\n",
       " 1997,\n",
       " 4828,\n",
       " 2007,\n",
       " 2608,\n",
       " 2039,\n",
       " 14995,\n",
       " 6924,\n",
       " 2007,\n",
       " 1996,\n",
       " 5722,\n",
       " 1000,\n",
       " 2310,\n",
       " 3490,\n",
       " 2618,\n",
       " 4748,\n",
       " 2033,\n",
       " 18168,\n",
       " 5267,\n",
       " 1000,\n",
       " 1012,\n",
       " 2279,\n",
       " 2000,\n",
       " 1996,\n",
       " 2364,\n",
       " 2311,\n",
       " 2003,\n",
       " 1996,\n",
       " 13546,\n",
       " 1997,\n",
       " 1996,\n",
       " 6730,\n",
       " 2540,\n",
       " 1012,\n",
       " 3202,\n",
       " 2369,\n",
       " 1996,\n",
       " 13546,\n",
       " 2003,\n",
       " 1996,\n",
       " 24665,\n",
       " 23052,\n",
       " 1010,\n",
       " 1037,\n",
       " 14042,\n",
       " 2173,\n",
       " 1997,\n",
       " 7083,\n",
       " 1998,\n",
       " 9185,\n",
       " 1012,\n",
       " 2009,\n",
       " 2003,\n",
       " 1037,\n",
       " 15059,\n",
       " 1997,\n",
       " 1996,\n",
       " 24665,\n",
       " 23052,\n",
       " 2012,\n",
       " 10223,\n",
       " 26371,\n",
       " 1010,\n",
       " 2605,\n",
       " 2073,\n",
       " 1996,\n",
       " 6261,\n",
       " 2984,\n",
       " 22353,\n",
       " 2135,\n",
       " 2596,\n",
       " 2000,\n",
       " 3002,\n",
       " 16595,\n",
       " 9648,\n",
       " 4674,\n",
       " 2061,\n",
       " 12083,\n",
       " 9711,\n",
       " 2271,\n",
       " 1999,\n",
       " 8517,\n",
       " 1012,\n",
       " 2012,\n",
       " 1996,\n",
       " 2203,\n",
       " 1997,\n",
       " 1996,\n",
       " 2364,\n",
       " 3298,\n",
       " 1006,\n",
       " 1998,\n",
       " 1999,\n",
       " 1037,\n",
       " 3622,\n",
       " 2240,\n",
       " 2008,\n",
       " 8539,\n",
       " 2083,\n",
       " 1017,\n",
       " 11342,\n",
       " 1998,\n",
       " 1996,\n",
       " 2751,\n",
       " 8514,\n",
       " 1007,\n",
       " 1010,\n",
       " 2003,\n",
       " 1037,\n",
       " 3722,\n",
       " 1010,\n",
       " 2715,\n",
       " 2962,\n",
       " 6231,\n",
       " 1997,\n",
       " 2984,\n",
       " 1012]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_contexts_tokenized[0].ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions_tokenized.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to',\n",
       " 'whom',\n",
       " 'did',\n",
       " 'the',\n",
       " 'virgin',\n",
       " 'mary',\n",
       " 'allegedly',\n",
       " 'appear',\n",
       " 'in',\n",
       " '1858',\n",
       " 'in',\n",
       " 'lou',\n",
       " '##rdes',\n",
       " 'france',\n",
       " '?']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions_tokenized[0].tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ws8c8_4d5UCI"
   },
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Xjooag-Swnuh"
   },
   "outputs": [],
   "source": [
    "class QA_Dataset(Dataset):\n",
    "    doc_stride = 50\n",
    "    \n",
    "    def __init__(self, split, answers, questions, tokenized_questions, tokenized_contexts, random_training_window=True):\n",
    "        self.split = split\n",
    "        self.answers = answers\n",
    "        self.questions = questions\n",
    "        self.tokenized_questions = tokenized_questions\n",
    "        self.tokenized_contexts = tokenized_contexts\n",
    "        self.random_training_window = random_training_window\n",
    "        self.max_question_len = 40\n",
    "        self.max_context_len = 150\n",
    "\n",
    "        # Input sequence length = [CLS] + question + [SEP] + context + [SEP]\n",
    "        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_context_len + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        answer = self.answers[idx]\n",
    "        question = self.questions[idx]\n",
    "        tokenized_question = self.tokenized_questions[idx]\n",
    "        tokenized_context = self.tokenized_contexts[idx]\n",
    "\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            # Convert answer's start/end positions in context_text to start/end positions in tokenized_context\n",
    "            answer_start = answer['answer_start']\n",
    "            answer_len = len(answer['text'])\n",
    "            answer_end = answer_start + answer_len - 1\n",
    "            \n",
    "            answer_start_token = tokenized_context.char_to_token(answer_start)\n",
    "            answer_end_token = tokenized_context.char_to_token(answer_end)\n",
    "\n",
    "            # A single window is obtained by slicing the portion of context containing the answer\n",
    "            if self.random_training_window:\n",
    "                context_start_min = max(0, answer_end_token - self.max_context_len + 1)\n",
    "                context_start_max = max(0, min(answer_start_token, len(tokenized_context) - self.max_context_len))\n",
    "                context_start = random.randint(context_start_min, context_start_max + 1)    \n",
    "            else:\n",
    "                mid = (answer_start_token + answer_end_token) // 2\n",
    "                context_start = max(0, min(mid - self.max_context_len // 2, len(tokenized_context) - self.max_context_len))\n",
    "                \n",
    "            context_end = context_start + self.max_context_len\n",
    "\n",
    "            # Slice question/context and add special tokens (101: CLS, 102: SEP)\n",
    "            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
    "            input_ids_context = tokenized_context.ids[context_start : context_end] + [102]\n",
    "\n",
    "            # Convert answer's start/end positions in tokenized_context to start/end positions in the window\n",
    "            answer_start_token += len(input_ids_question) - context_start\n",
    "            answer_end_token += len(input_ids_question) - context_start\n",
    "\n",
    "            # Pad sequence and obtain inputs to model\n",
    "            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_context)\n",
    "            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n",
    "\n",
    "        # Validation/Testing\n",
    "        else:\n",
    "            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n",
    "\n",
    "            # context is split into several windows, each with start positions separated by step \"doc_stride\"\n",
    "            for i in range(0, len(tokenized_context), self.doc_stride):\n",
    "\n",
    "                # Slice question/context and add special tokens (101: CLS, 102: SEP)\n",
    "                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
    "                input_ids_context = tokenized_context.ids[i : i + self.max_context_len] + [102]\n",
    "\n",
    "                # Pad sequence and obtain inputs to model\n",
    "                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_context)\n",
    "\n",
    "                input_ids_list.append(input_ids)\n",
    "                token_type_ids_list.append(token_type_ids)\n",
    "                attention_mask_list.append(attention_mask)\n",
    "\n",
    "            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n",
    "\n",
    "    def padding(self, input_ids_question, input_ids_context):\n",
    "        # Pad zeros if sequence length is shorter than max_seq_len\n",
    "        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_context)\n",
    "        # Indices of input sequence tokens in the vocabulary\n",
    "        input_ids = input_ids_question + input_ids_context + [0] * padding_len\n",
    "        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n",
    "        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_context) + [0] * padding_len\n",
    "        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
    "        attention_mask = [1] * (len(input_ids_question) + len(input_ids_context)) + [0] * padding_len\n",
    "\n",
    "        return input_ids, token_type_ids, attention_mask\n",
    "\n",
    "train_set = QA_Dataset(\"train\", train_answers, train_questions, train_questions_tokenized, train_contexts_tokenized, random_training_window=True)\n",
    "# dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_contexts_tokenized)\n",
    "# test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_contexts_tokenized)\n",
    "\n",
    "train_batch_size = 16\n",
    "\n",
    "# Note: Do NOT change batch size of dev_loader / test_loader !\n",
    "# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair\n",
    "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
    "# dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "# test_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 101, 2040, 2180,  ...,    0,    0,    0],\n",
       "         [ 101, 1999, 2054,  ...,    0,    0,    0],\n",
       "         [ 101, 2054, 2106,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 2054, 2003,  ...,    0,    0,    0],\n",
       "         [ 101, 2054, 2003,  ...,    0,    0,    0],\n",
       "         [ 101, 2043, 2003,  ...,    0,    0,    0]]),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " tensor([ 83,  20, 167, 149,  52,  23,  65,  81,  71,  44,  35,  44,  32,  61,\n",
       "         150, 111]),\n",
       " tensor([ 91,  21, 171, 152,  54,  23,  82,  82,  72,  45,  53,  46,  34,  65,\n",
       "         154, 111])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_H1kqhR8CdM"
   },
   "source": [
    "## Function for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "id": "SqeA3PLPxOHu"
   },
   "outputs": [],
   "source": [
    "def evaluate(data, output, split=None, question_id=None):\n",
    "\n",
    "    answer = ''\n",
    "    answer_start_idx = -1\n",
    "    answer_end_idx = -1\n",
    "    max_prob = float('-inf')\n",
    "    num_of_windows = data[0].shape[1]\n",
    "\n",
    "    for k in range(num_of_windows):\n",
    "        # Obtain answer by choosing the most probable start position / end position\n",
    "        start_prob, start_index = torch.max(output.start_logits[k], dim=0)\n",
    "        end_prob, end_index = torch.max(output.end_logits[k], dim=0)\n",
    "        \n",
    "        token_type_id = data[1][0][k].detach().cpu().numpy()\n",
    "        # [CLS] + [question] + [SEP] + [paragraph] + [SEP]\n",
    "        paragraph_start = token_type_id.argmax()\n",
    "        paragraph_end = len(token_type_id) - 1 - token_type_id[::-1].argmax()\n",
    "        \n",
    "        if start_index > end_index or start_index < paragraph_start or end_index > paragraph_end :\n",
    "            continue\n",
    "            \n",
    "\n",
    "        # Probability of answer is calculated as sum of start_prob and end_prob\n",
    "        prob = start_prob + end_prob\n",
    "\n",
    "        # Replace answer if calculated probability is larger than previous windows\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            # Convert tokens to chars (e.g. [1920, 7032] --> \"大 金\")\n",
    "            answer = tokenizer.decode(data[0][0][k][start_index : end_index + 1])\n",
    "            answer_start_idx = start_index\n",
    "            answer_end_idx = end_index\n",
    "            \n",
    "            origin_start = start_index - paragraph_start + k * QA_Dataset.doc_stride \n",
    "            origin_end = end_index - paragraph_start + k * QA_Dataset.doc_stride \n",
    "            \n",
    "    answer = answer.replace(' ','')\n",
    "\n",
    "    if '[UNK]' in answer:\n",
    "        # find the original subtext based on the start and end index, replace the answer with the subtext\n",
    "        if split == 'dev':\n",
    "            print(dev_questions[question_id][\"answer_text\"])\n",
    "            paragraph_tokenized = dev_paragraphs_tokenized[dev_questions[question_id][\"paragraph_id\"]]\n",
    "            raw_start = paragraph_tokenized.token_to_chars(origin_start)[0]\n",
    "            raw_end = paragraph_tokenized.token_to_chars(origin_end)[1]\n",
    "            answer = dev_paragraphs[dev_questions[question_id][\"paragraph_id\"]][raw_start:raw_end]\n",
    "            print(answer)\n",
    "        if split == 'test':\n",
    "            paragraph_tokenized = test_paragraphs_tokenized[test_questions[question_id][\"paragraph_id\"]]\n",
    "            raw_start = paragraph_tokenized.token_to_chars(origin_start)[0]\n",
    "            raw_end = paragraph_tokenized.token_to_chars(origin_end)[1]\n",
    "            answer = test_paragraphs[test_questions[question_id][\"paragraph_id\"]][raw_start:raw_end]\n",
    "    \n",
    "\n",
    "    return answer, answer_start_idx, answer_end_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzHQit6eMnKG"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "id": "3Q-B6ka7xoCM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training ...\n",
      "Evaluating Dev Set ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a619ee33ec478aa56d9c7d2cb1c10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "番人的反抗\n",
      "大肚平埔族拍布拉族大肚王與瑯嶠番人的反抗\n",
      "約旦的Jawa大壩\n",
      "Jawa大壩\n",
      "《阿闥婆吠陀》\n",
      "《阿闥婆吠陀》\n",
      "姚萇\n",
      "姚萇\n",
      "淝水之戰\n",
      "淝水之戰\n",
      "金堉\n",
      "金堉\n",
      "P-61\n",
      "P-61戰鬥機\n",
      "奕訢\n",
      "奕訢\n",
      "撣族\n",
      "撣族\n",
      "3000垓\n",
      "3000垓顆\n",
      "共產黨情報局\n",
      "史達林設置用於控制東歐共產黨的機關—共產黨情報局\n",
      "祕色瓷\n",
      "——祕色瓷\n",
      "128\n",
      "512 KB\n",
      "ANP\n",
      "ANP\n",
      "T.C.B.S\n",
      "T.C.B.S\n",
      "收買人心\n",
      "Matamoros自治市\n",
      "紅河戰役\n",
      "Palmito Ranch戰役\n",
      "LOEN娛樂\n",
      "韓國第三大財閥SK集團\n",
      "回鶻汗國\n",
      "回鶻汗國\n",
      "白堊紀\n",
      "白堊紀\n",
      "羅塞塔號的菲萊登陸器\n",
      "菲萊登陸器於2014年11月12日在彗星上登陸，就是67P/楚留莫夫－格拉希門克彗星\n",
      "F-16戰鬥機\n",
      "F-16戰鬥機\n",
      "白堊紀中期\n",
      "白堊紀中期\n",
      "回鶻部落\n",
      "回鶻部落\n",
      "白堊紀\n",
      "白堊紀\n",
      "小綠人\n",
      "KTV\n",
      "HK G3自動步槍系列\n",
      "HK G3自動步槍\n",
      "Mosaic網頁瀏覽器\n",
      "Mosaic網頁瀏覽器\n",
      "製糖會社\n",
      "李俊俋\n",
      "TR-850型\n",
      "TR-850\n",
      "普賢菩薩\n",
      "—樂山大佛\n",
      "「Harvey獎」\n",
      "Harvey獎\n",
      "蔡鍔\n",
      "蔡鍔\n",
      "A型機動腳踏車\n",
      "本田A型機動腳踏車\n",
      "一種導入人工智慧化的新世代BRT系統\n",
      "導入人工智慧化的新世代BRT系統\n",
      "久彌宮妃俔子\n",
      "久彌宮妃俔子\n",
      "《BBC早餐》\n",
      "《BBC早餐》\n",
      "《阿闥婆吠陀》\n",
      "《阿闥婆吠陀》\n",
      "全球資訊網專案簡介\n",
      "Mosaic網頁瀏覽器\n",
      "使用過時的二衝程發動機\n",
      "其中含有烴，廢氣具有明顯氣味和顏色\n",
      "盤瓠蠻\n",
      "盤瓠蠻\n",
      "李勣\n",
      "李勣\n",
      "玄奘法師\n",
      "支婁迦讖\n",
      "Validation | Epoch 1 | acc = 0.000\n",
      "Saving Model ...\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 1\n",
    "validation = True\n",
    "logging_step = 100\n",
    "learning_rate = 1e-4\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_step = len(train_loader) * num_epoch\n",
    "\n",
    "linear_learning_rate_decay = False\n",
    "schedule_with_warm_up = True\n",
    "\n",
    "if linear_learning_rate_decay and schedule_with_warm_up:\n",
    "    linear_learning_rate_decay = False\n",
    "\n",
    "if schedule_with_warm_up:\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=total_step//10, num_training_steps=total_step)\n",
    "\n",
    "if fp16_training:\n",
    "    if schedule_with_warm_up:\n",
    "        model, optimizer, train_loader, lr_scheduler = accelerator.prepare(model,\n",
    "                                                                           optimizer, \n",
    "                                                                           train_loader,\n",
    "                                                                           scheduler)\n",
    "    else:\n",
    "        model, optimizer, train_loader = accelerator.prepare(model, \n",
    "                                                             optimizer, \n",
    "                                                             train_loader)\n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"Start Training ...\")\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    step = 1\n",
    "    train_loss = train_acc = 0\n",
    "\n",
    "    for data in tqdm(train_loader):\n",
    "        # Load all data into GPU\n",
    "        data = [i.to(device) for i in data]\n",
    "\n",
    "        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n",
    "        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)\n",
    "        output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n",
    "\n",
    "        # Choose the most probable start position / end position\n",
    "        start_index = torch.argmax(output.start_logits, dim=1)\n",
    "        end_index = torch.argmax(output.end_logits, dim=1)\n",
    "\n",
    "        # Prediction is correct only if both start_index and end_index are correct\n",
    "        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n",
    "        train_loss += output.loss\n",
    "\n",
    "        if fp16_training:\n",
    "            accelerator.backward(output.loss)\n",
    "        else:\n",
    "            output.loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        if linear_learning_rate_decay:\n",
    "            optimizer.param_groups[0]['lr'] -= learning_rate / total_step \n",
    "        if schedule_with_warm_up:\n",
    "            scheduler.step()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        step += 1\n",
    "\n",
    "\n",
    "        # Print training loss and accuracy over past logging step\n",
    "        if step % logging_step == 0:\n",
    "            print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f}\")\n",
    "            train_loss = train_acc = 0\n",
    "\n",
    "    if validation:\n",
    "        print(\"Evaluating Dev Set ...\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dev_acc = 0\n",
    "            for i, data in enumerate(tqdm(dev_loader)):\n",
    "                output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "                # prediction is correct only if answer text exactly matches\n",
    "                dev_acc += evaluate(data, \n",
    "                                    output, \n",
    "                                    split='dev', \n",
    "                                    question_id=i\n",
    "                                   ) == dev_questions[i][\"answer_text\"]\n",
    "            print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n",
    "        model.train()\n",
    "\n",
    "# Save a model and its configuration file to the directory 「saved_model」\n",
    "# i.e. there are two files under the direcory 「saved_model」: 「pytorch_model.bin」 and 「config.json」\n",
    "# Saved model can be re-loaded using 「model = BertForQuestionAnswering.from_pretrained(\"saved_model\")」\n",
    "print(\"Saving Model ...\")\n",
    "model_save_dir = \"saved_model\"\n",
    "model.save_pretrained(model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMmdLOKBMsdE"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "id": "U5scNKC9xz0C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Test Set ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baaf0180d3274f1797121b9e818768c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed! Result is in results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating Test Set ...\")\n",
    "\n",
    "results = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(tqdm(test_loader)):\n",
    "        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "        result, start_idx, end_idx = evaluate(data, \n",
    "                                              output, \n",
    "                                              split='test', \n",
    "                                              question_id=i)\n",
    "        # print(f\"question {len(results)}, start:{start_idx}, end:{end_idx}\" )\n",
    "        results.append(result)\n",
    "\n",
    "result_file = \"results.csv\"\n",
    "with open(result_file, 'w') as f:\n",
    "\t  f.write(\"ID,Answer\\n\")\n",
    "\t  for i, test_question in enumerate(test_questions):\n",
    "        # Replace commas in answers with empty strings (since csv is separated by comma)\n",
    "        # Answers in kaggle are processed in the same way\n",
    "\t\t    f.write(f\"{test_question['id']},{results[i].replace(',','')}\\n\")\n",
    "\n",
    "print(f\"Completed! Result is in {result_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "hw7_bert",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:llm]",
   "language": "python",
   "name": "conda-env-llm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
